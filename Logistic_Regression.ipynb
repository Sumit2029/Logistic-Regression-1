{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        " 1. What is Logistic Regression, and how does it differ from Linear Regression.\n",
        "-> Logistic Regression is a supervised learning algorithm used for classification tasks, whereas Linear Regression is used for regression tasks. Logistic Regression predicts probabilities of an event occurring, whereas Linear Regression predicts continuous values.\n",
        "\n",
        "2. What is the mathematical equation of Logistic Regression.\n",
        "-> The mathematical equation of Logistic Regression is given by:\n",
        "         p = 1 / (1 + e^(-z))\n",
        "where p is the probability of the event occurring, e is the base of the natural logarithm, and z is a linear combination of the input features.\n",
        "\n",
        "3. Why do we use the Sigmoid function in Logistic Regression.\n",
        "-> The Sigmoid function, also known as the logistic function, is used in Logistic Regression to map the input values to a probability between 0 and 1. This is necessary because the output of the logistic regression model should be a probability value.\n",
        "\n",
        "4. What is the cost function of Logistic Regression.\n",
        "-> The cost function of Logistic Regression is given by:\n",
        "             J(θ) = -[y * log(h(x)) + (1-y) * log(1-h(x))]\n",
        "where θ represents the model parameters, y is the actual output, h(x) is the predicted probability, and log is the natural logarithm.\n",
        "\n",
        "5. What is Regularization in Logistic Regression? Why is it needed.\n",
        "-> Regularization is a technique used to prevent overfitting in Logistic Regression. It adds a penalty term to the cost function to discourage large weights. Regularization is needed to reduce the complexity of the model and improve its generalization performance.\n",
        "\n",
        "6. Explain the difference between Lasso, Ridge, and Elastic Net regressionC\n",
        "-> Lasso regression uses L1 regularization, which adds a term to the cost function proportional to the absolute value of the model weights. Ridge regression uses L2 regularization, which adds a term to the cost function proportional to the square of the model weights. Elastic Net regression combines both L1 and L2 regularization.\n",
        "\n",
        "7. When should we use Elastic Net instead of Lasso or Ridge.\n",
        "-> Elastic Net regression is useful when there are multiple features that are correlated with each other. It can select groups of correlated features, whereas Lasso regression may select only one feature from the group.\n",
        "\n",
        "8. What is the impact of the regularization parameter (λ) in Logistic Regression.\n",
        "-> The regularization parameter (λ) controls the strength of the penalty term in the cost function. A high value of λ can lead to underfitting, while a low value of λ can lead to overfitting.\n",
        "\n",
        "9. What are the key assumptions of Logistic Regression.\n",
        "-> The key assumptions of Logistic Regression are:\n",
        "a). The relationship between the independent variables and the log-odds of the dependent variable is linear.\n",
        "b). The observations are independent of each other.\n",
        "c). There is no multicollinearity between the independent variables.\n",
        "\n",
        "10. What are some alternatives to Logistic Regression for classification tasks.\n",
        "-> Some alternatives to Logistic Regression for classification tasks are:\n",
        "a). Decision Trees\n",
        "b). Random Forest\n",
        "c). Support Vector Machines (SVMs)\n",
        "d). K-Nearest Neighbors (KNN)\n",
        "\n",
        "11. What are Classification Evaluation Metrics.\n",
        "->Some common classification evaluation metrics are:\n",
        "a). Accuracy\n",
        "b) Precision\n",
        "c) Recall\n",
        "d) F1-score\n",
        "e) Area under the ROC curve (AUC-ROC)\n",
        "\n",
        "12. How does class imbalance affect Logistic Regression.\n",
        "-> Class imbalance can affect the performance of Logistic Regression. Techniques such as oversampling the minority class, undersampling the majority class, or using class weights can be used to handle class imbalance.\n",
        "\n",
        "13. What is Hyperparameter Tuning in Logistic Regression.\n",
        "-> Hyperparameter tuning involves selecting the best hyperparameters for the model, such as the regularization parameter (λ) or the solver.\n",
        "\n",
        "14. What are different solvers in Logistic Regression? Which one should be used.\n",
        "-> Some common solvers in Logistic Regression are:\n",
        "a) Liblinear\n",
        "b) Newton-CG\n",
        "c) LBFGS\n",
        "The choice of solver depends on the specific problem and dataset.\n",
        "\n",
        "15. How is Logistic Regression extended for multiclass classification.\n",
        "-> Logistic Regression can be extended for multiclass classification using techniques such as:\n",
        "a) One-vs-Rest (OvR)\n",
        "b) One-vs-One (OvO)\n",
        "c) Softmax regression\n",
        "\n",
        "16. What are the advantages and disadvantages of Logistic Regression.\n",
        "-> The advantages of Logistic Regression are:\n",
        "a) Simple to implement\n",
        "b) Interpretable results\n",
        "c) Can handle binary and multiclass classification\n",
        "The disadvantages of Logistic Regression are:\n",
        "a) Assumes linear relationship between independent variables and log-odds\n",
        "b) Can suffer from overfitting\n",
        "\n",
        "17. What are some use cases of Logistic Regression.\n",
        "-> Some common use cases of Logistic Regression are:\n",
        "a) Credit risk assessment\n",
        "b) Medical diagnosis\n",
        "c) Customer churn prediction\n",
        "\n",
        "18. What is the difference between Softmax Regression and Logistic Regression.\n",
        "-> Softmax regression is an extension of Logistic Regression for multiclass classification. It predicts probabilities for each class using the softmax function\n",
        "\n",
        "19. How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification.\n",
        "-> The choice between OvR and Softmax depends on the specific problem and dataset. OvR is simpler to implement, while Softmax regression can provide more accurate results.\n",
        "\n",
        "20. How do we interpret coefficients in Logistic Regression?\n",
        "-> The coefficients in Logistic Regression represent the change in the log-odds of the dependent variable for a one-unit change in the independent variable, while holding all other independent variables"
      ],
      "metadata": {
        "id": "_ehXdnjnfqBl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFCFp3zKUJXq",
        "outputId": "11953b12-f2a7-418f-fb27-08b3ffd0282e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0\n"
          ]
        }
      ],
      "source": [
        "#1Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic Regression, and prints the model accuracy\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate model accuracy\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2 Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1') and print the model accuracyC\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression model with L1 regularization\n",
        "model = LogisticRegression(penalty='l1', solver='liblinear', max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate model accuracy\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zhntow8gUtZH",
        "outputId": "b91d77dd-d724-46e3-efb4-1707a6341353"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3C Write a Python program to train Logistic Regression with L2 regularization (Ridge) using LogisticRegression(penalty='l2'). Print model accuracy and coefficientsC\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression model with L2 regularization\n",
        "model = LogisticRegression(penalty='l2', max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate model accuracy\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "print(\"Model Coefficients:\", model.coef_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8T_ldBzTVEra",
        "outputId": "46a50004-1959-4a69-9f3e-ad436e5de45b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0\n",
            "Model Coefficients: [[-0.39345607  0.96251768 -2.37512436 -0.99874594]\n",
            " [ 0.50843279 -0.25482714 -0.21301129 -0.77574766]\n",
            " [-0.11497673 -0.70769055  2.58813565  1.7744936 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4C Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet')\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression model with Elastic Net regularization\n",
        "model = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5, max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate model accuracy\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7MzzRYCpVxoB",
        "outputId": "a3b8e621-f91b-4267-a392-7f5d701b4ab4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#5Write a Python program to train a Logistic Regression model for multiclass classification using multi_class='ovr'C\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression model for multiclass classification\n",
        "model = LogisticRegression(multi_class='ovr', max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate model accuracy\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-SThnMfV554",
        "outputId": "46f07ea5-96b2-44d3-ff92-8f1c46591310"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.9666666666666667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#6 C Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic Regression. Print the best parameters and accuracy\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'penalty': ['l1', 'l2']\n",
        "}\n",
        "\n",
        "# Initialize Logistic Regression model\n",
        "logreg = LogisticRegression(max_iter=10000)\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=logreg, param_grid=param_grid, cv=5)\n",
        "\n",
        "# Perform grid search\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print best parameters and accuracy\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Best Accuracy:\", grid_search.best_score_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWvA0o3eWN63",
        "outputId": "b0739d9d-b879-45e6-d9f0-a8afc3362d9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:528: FitFailedWarning: \n",
            "15 fits failed out of a total of 30.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "15 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1193, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 63, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py:1108: UserWarning: One or more of the test scores are non-finite: [       nan 0.93333333        nan 0.96666667        nan 0.94166667]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': 1, 'penalty': 'l2'}\n",
            "Best Accuracy: 0.9666666666666666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#7Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the average accuracyC\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Initialize Logistic Regression model\n",
        "logreg = LogisticRegression(max_iter=10000)\n",
        "\n",
        "# Initialize Stratified K-Fold Cross-Validation\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Perform cross-validation\n",
        "scores = cross_val_score(logreg, X, y, cv=skf)\n",
        "\n",
        "# Print average accuracy\n",
        "print(\"Average Accuracy:\", scores.mean())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdpoM0VHXG1l",
        "outputId": "2fcc8df7-e921-4692-b9d1-e6715a3d863e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Accuracy: 0.9666666666666668\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#8Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its accuracy.\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import warnings\n",
        "\n",
        "warnings.warn(\"warning\")\n",
        "\n",
        "# Load dataset from CSV file\n",
        "df = pd.read_csv('your_dataset.csv')\n",
        "\n",
        "# Assume 'target' is the target variable and the rest are features\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# Split dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Logistic Regression model\n",
        "logreg = LogisticRegression(max_iter=10000)\n",
        "\n",
        "# Train model\n",
        "logreg.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = logreg.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "E3CwOQd8XfOE",
        "outputId": "0453de42-4f5d-43e3-a80e-9265dfd50387"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-32753ab39734>:8: UserWarning: warning\n",
            "  warnings.warn(\"warning\")\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-32753ab39734>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Assume 'target' is the target variable and the rest are features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#9Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in Logistic Regression. Print the best parameters and accuracyM\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define hyperparameter distribution\n",
        "param_dist = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear', 'saga']\n",
        "}\n",
        "\n",
        "# Initialize Logistic Regression model\n",
        "logreg = LogisticRegression(max_iter=10000)\n",
        "\n",
        "# Initialize RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(estimator=logreg, param_distributions=param_dist, cv=5, n_iter=10)\n",
        "\n",
        "# Perform random search\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Print best parameters and accuracy\n",
        "print(\"Best Parameters:\", random_search.best_params_)\n",
        "print(\"Best Accuracy:\", random_search.best_score_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1JtL9VUAX62E",
        "outputId": "3029dd87-7d62-4ecd-a00b-39d895542ad1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'solver': 'saga', 'penalty': 'l2', 'C': 10}\n",
            "Best Accuracy: 0.9666666666666668\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#10 Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracyM\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Logistic Regression model\n",
        "logreg = LogisticRegression(max_iter=10000)\n",
        "\n",
        "# Initialize OneVsOneClassifier\n",
        "ovo = OneVsOneClassifier(logreg)\n",
        "\n",
        "# Train model\n",
        "ovo.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = ovo.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tlIn7aHybIR8",
        "outputId": "38649792-f1b5-455d-ff96-de36b896dd59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#11Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary classification\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Generate binary classification dataset\n",
        "X, y = make_classification(n_samples=100, n_features=2, n_informative=2, n_redundant=0, n_classes=2)\n",
        "\n",
        "# Split dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Logistic Regression model\n",
        "logreg = LogisticRegression(max_iter=10000)\n",
        "\n",
        "# Train model\n",
        "logreg.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = logreg.predict(X_test)\n",
        "\n",
        "# Calculate confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualize confusion matrix\n",
        "plt.figure(figsize=(6, 6))\n",
        "sns.heatmap(cm, annot=True, cmap='Blues')\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "DeqQAu-XbRMa",
        "outputId": "b981e1f7-226a-46f3-9c72-c7f0ad1ffef9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#12M Write a Python program to train a Logistic Regression model and evaluate its performance using Precision, Recall, and F1-ScoreM\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Load iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Logistic Regression model\n",
        "logreg = LogisticRegression(max_iter=10000)\n",
        "\n",
        "# Train model\n",
        "logreg.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = logreg.predict(X_test)\n",
        "\n",
        "# Evaluate model performance\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "print(\"Precision:\", precision)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "usG7RZKAba3R",
        "outputId": "10a83d17-63ee-42a9-dbcb-c1f9ccb5194c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#13M Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to improve model performance\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Generate imbalanced dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_classes=2, weights=[0.9, 0.1])\n",
        "\n",
        "# Split dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Logistic Regression model with class weights\n",
        "logreg = LogisticRegression(max_iter=10000, class_weight='balanced')\n",
        "\n",
        "# Train model\n",
        "logreg.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = logreg.predict(X_test)\n",
        "\n",
        "# Evaluate model performance\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nzz4mvz6bqap",
        "outputId": "2539940d-9a22-4a59-96b8-0bbe840c4bb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.935\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.93      0.96       168\n",
            "           1       0.72      0.97      0.83        32\n",
            "\n",
            "    accuracy                           0.94       200\n",
            "   macro avg       0.86      0.95      0.89       200\n",
            "weighted avg       0.95      0.94      0.94       200\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#14Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and evaluate performance\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Load Titanic dataset\n",
        "df = pd.read_csv('titanic.csv')\n",
        "\n",
        "# Handle missing values\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "df[['Age']] = imputer.fit_transform(df[['Age']])\n",
        "\n",
        "# Split dataset into features and target\n",
        "X = df[['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']]\n",
        "y = df['Survived']\n",
        "\n",
        "# Split dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Logistic Regression model\n",
        "logreg = LogisticRegression(max_iter=10000)\n",
        "\n",
        "# Train model\n",
        "logreg.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = logreg.predict(X_test)\n",
        "\n",
        "# Evaluate model performance\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "ykLq2AJ1cFnY",
        "outputId": "e39b11f1-1ee8-4a5d-a8d4-12059b7422d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'titanic.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-7130f8558839>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Load Titanic dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'titanic.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Handle missing values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'titanic.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#15 M Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression model. Evaluate its accuracy and compare results with and without scalingM\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Apply feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Initialize Logistic Regression model\n",
        "logreg = LogisticRegression(max_iter=10000)\n",
        "\n",
        "# Train model on scaled data\n",
        "logreg.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = logreg.predict(X_test_scaled)\n",
        "print(\"Accuracy (scaled):\", accuracy_score(y_test, y_pred_scaled))\n",
        "\n",
        "# Train model on unscaled data\n",
        "logreg.fit(X_train, y_train)\n",
        "y_pred_unscaled = logreg.predict(X_test)\n",
        "print(\"Accuracy (unscaled):\", accuracy_score(y_test, y_pred_unscaled))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrF9QZ0ecSf3",
        "outputId": "48024ad1-a938-409f-aa0f-04001e4f1fae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (scaled): 1.0\n",
            "Accuracy (unscaled): 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#16Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Load breast cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Logistic Regression model\n",
        "logreg = LogisticRegression(max_iter=10000)\n",
        "\n",
        "# Train model\n",
        "logreg.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_proba = logreg.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Evaluate model performance using ROC-AUC score\n",
        "print(\"ROC-AUC Score:\", roc_auc_score(y_test, y_pred_proba))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mSBzZFlclwg",
        "outputId": "478785fc-0665-451f-9fc1-2cb3befd2e93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 0.9977071732721913\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#17Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate accuracy\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Logistic Regression model with custom learning rate (C=0.5)\n",
        "logreg = LogisticRegression(C=0.5, max_iter=10000)\n",
        "\n",
        "# Train model\n",
        "logreg.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = logreg.predict(X_test)\n",
        "\n",
        "# Evaluate model accuracy\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lok3IKlDc8wV",
        "outputId": "dcfc91b8-c8e6-45e6-8ad3-b2bf35fe1d23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#18 Write a Python program to train Logistic Regression and identify important features based on model coefficientsM\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Load iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "feature_names = iris.feature_names\n",
        "\n",
        "# Split dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Logistic Regression model\n",
        "logreg = LogisticRegression(max_iter=10000)\n",
        "\n",
        "# Train model\n",
        "logreg.fit(X_train, y_train)\n",
        "\n",
        "# Get model coefficients\n",
        "coefficients = logreg.coef_\n",
        "\n",
        "# Identify important features\n",
        "for i, class_name in enumerate(iris.target_names):\n",
        "    print(f\"Class: {class_name}\")\n",
        "    feature_importances = sorted(zip(feature_names, coefficients[i]), key=lambda x: abs(x[1]), reverse=True)\n",
        "    for feature, importance in feature_importances:\n",
        "        print(f\"  {feature}: {importance:.3f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tIx_sgOdRHV",
        "outputId": "68c9eadd-0276-42d8-a7aa-28b528b40972"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class: setosa\n",
            "  petal length (cm): -2.375\n",
            "  petal width (cm): -0.999\n",
            "  sepal width (cm): 0.963\n",
            "  sepal length (cm): -0.393\n",
            "Class: versicolor\n",
            "  petal width (cm): -0.776\n",
            "  sepal length (cm): 0.508\n",
            "  sepal width (cm): -0.255\n",
            "  petal length (cm): -0.213\n",
            "Class: virginica\n",
            "  petal length (cm): 2.588\n",
            "  petal width (cm): 1.774\n",
            "  sepal width (cm): -0.708\n",
            "  sepal length (cm): -0.115\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#19 Write a Python program to train Logistic Regression and evaluate its performance using Cohen’s Kappa ScoreM\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "# Load iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Logistic Regression model\n",
        "logreg = LogisticRegression(max_iter=10000)\n",
        "\n",
        "# Train model\n",
        "logreg.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = logreg.predict(X_test)\n",
        "\n",
        "# Evaluate model performance using Cohen's Kappa Score\n",
        "kappa_score = cohen_kappa_score(y_test, y_pred)\n",
        "print(\"Cohen's Kappa Score:\", kappa_score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbxUUoU8deX0",
        "outputId": "cf96662f-e4ee-4e7e-f85f-c57e90f5d66c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cohen's Kappa Score: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#20 Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary classificatio:\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_recall_curve, auc\n",
        "\n",
        "# Generate binary classification dataset\n",
        "X, y = make_classification(n_samples=100, n_features=2, n_informative=2, n_redundant=0, n_classes=2)\n",
        "\n",
        "# Split dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Logistic Regression model\n",
        "logreg = LogisticRegression(max_iter=10000)\n",
        "\n",
        "# Train model\n",
        "logreg.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_proba = logreg.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate precision and recall\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
        "\n",
        "# Calculate AUC\n",
        "auc_score = auc(recall, precision)\n",
        "\n",
        "# Visualize Precision-Recall Curve\n",
        "plt.plot(recall, precision)\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.title(f\"Precision-Recall Curve (AUC={auc_score:.3f})\")\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "a89eJaYfdoTL",
        "outputId": "7efca311-14d5-4a70-913c-3bf34d8b33b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#21 Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare their accuracyM\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define solvers\n",
        "solvers = ['liblinear', 'saga', 'lbfgs']\n",
        "\n",
        "# Train Logistic Regression with different solvers\n",
        "for solver in solvers:\n",
        "    logreg = LogisticRegression(max_iter=10000, solver=solver)\n",
        "    logreg.fit(X_train, y_train)\n",
        "    y_pred = logreg.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Solver: {solver}, Accuracy: {accuracy:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHIouKBSd08b",
        "outputId": "cfcc30c0-6073-4921-a27f-a2c65ec0e1e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solver: liblinear, Accuracy: 1.000\n",
            "Solver: saga, Accuracy: 1.000\n",
            "Solver: lbfgs, Accuracy: 1.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#22Write a Python program to train Logistic Regression and evaluate its performance using Matthews Correlation Coefficient (MCC)M\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "# Load breast cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Logistic Regression model\n",
        "logreg = LogisticRegression(max_iter=10000)\n",
        "\n",
        "# Train model\n",
        "logreg.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = logreg.predict(X_test)\n",
        "\n",
        "# Evaluate model performance using MCC\n",
        "mcc = matthews_corrcoef(y_test, y_pred)\n",
        "print(\"Matthews Correlation Coefficient:\", mcc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Fj6vrEAeKmi",
        "outputId": "2cc95161-aab9-44f9-b273-29e4254d9912"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matthews Correlation Coefficient: 0.9068106119605033\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#23Write a Python program to train Logistic Regression on both raw and standardized data. Compare their accuracy to see the impact of feature scalingM\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression on raw data\n",
        "logreg = LogisticRegression(max_iter=10000)\n",
        "logreg.fit(X_train, y_train)\n",
        "y_pred_raw = logreg.predict(X_test)\n",
        "accuracy_raw = accuracy_score(y_test, y_pred_raw)\n",
        "\n",
        "# Standardize data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Logistic Regression on standardized data\n",
        "logreg.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = logreg.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "print(f\"Accuracy (raw data): {accuracy_raw:.3f}\")\n",
        "print(f\"Accuracy (standardized data): {accuracy_scaled:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5EZsvIIYeUZC",
        "outputId": "582b5b42-0fd8-4306-96c7-f7c22fe80ad6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (raw data): 1.000\n",
            "Accuracy (standardized data): 1.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#24M Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using cross-validation\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Load iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid = {'C': [0.1, 1, 10]}\n",
        "\n",
        "# Initialize Logistic Regression model\n",
        "logreg = LogisticRegression(max_iter=10000)\n",
        "\n",
        "# Perform grid search\n",
        "grid_search = GridSearchCV(logreg, param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print optimal C\n",
        "print(\"Optimal C:\", grid_search.best_params_['C'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-QQV6LeHeeAB",
        "outputId": "208cd796-0207-4bf8-a0fd-2452cebbdded"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal C: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#25AM Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to make predictions.\n",
        "import joblib\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Load iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Logistic Regression model\n",
        "logreg = LogisticRegression(max_iter=10000)\n",
        "\n",
        "# Train model\n",
        "logreg.fit(X_train, y_train)\n",
        "\n",
        "# Save trained model using joblib\n",
        "joblib.dump(logreg, 'logreg_model.joblib')\n",
        "\n",
        "# Load saved model\n",
        "loaded_logreg = joblib.load('logreg_model.joblib')\n",
        "\n",
        "# Make predictions using loaded model\n",
        "y_pred = loaded_logreg.predict(X_test)\n",
        "\n",
        "# Print predictions\n",
        "print(\"Predictions:\", y_pred)\n",
        "\n",
        "# Evaluate model accuracy\n",
        "accuracy = loaded_logreg.score(X_test, y_test)\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIsh_KpwepgZ",
        "outputId": "6b87ac77-8160-41fc-a1ca-61ebb3899e56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions: [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n",
            "Model Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "39PqgpsEfL5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5Ih10dSTfjeG"
      }
    }
  ]
}